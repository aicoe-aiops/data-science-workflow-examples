{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow - Simple Example\n",
    "\n",
    "This notebooks provides a simple example of using the [AI Ops MLflow instance](http://mlflow-server-route-aiops-prod-prometheus-scrape.cloud.paas.psi.redhat.com/) to track runs for a new project. Specifically, how to log parameters and metrics.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up connection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Flow helps track experiments and runs within a particular data science project. As per MLFlow tracking conventions :\n",
    "\n",
    "- An **experiment** corresponds to a particular version of code or model. Each experiment will be tracked as different commits within git.\n",
    "\n",
    "- A **run** corresponds to a different hyper parameter setting or different feature engineering within same experiment. Runs are tracked within each ML Flow experiment\n",
    "\n",
    "1) Set up the ML Flow client and point it to our current MLFlow deployment : http://mlflow-server-route-aiops-prod-prometheus-scrape.cloud.paas.psi.redhat.com/#/\n",
    "\n",
    "2) Start a new MLflow run, setting it as the active run under which metrics and parameters will be logged\n",
    "\n",
    "3) Set an experiment for tracking a particular project \n",
    "\n",
    "4) Each run corresponds to a training cycle. Associate run name to version of code (experiment)\n",
    "\n",
    "5) Start a run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection configurations set by the user:\n",
    "\n",
    "* **MLFLOW_URI**: route to deployment\n",
    "* **EXPERIMENT_NAME**: Name for entire set of experiments you want to compare. Think of this as the folder this run will live in or a particular version of the code or model.\n",
    "* **RUN_NAME**: custom name for this specific run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations set by the user\n",
    "\n",
    "MLFLOW_URI = 'http://mlflow-server-route-aiops-prod-prometheus-scrape.cloud.paas.psi.redhat.com' #route to deployment\n",
    "EXPERIMENT_NAME = 'AIOps_tracking_test_0.1'# Name for entire set of experiments you want to compare. \n",
    "RUN_NAME = \"template_example\" # custom name for this specific run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to MLflow and start tracking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to mlflow \n",
    "\n",
    "MLFLOW_CLIENT = mlflow.tracking.MlflowClient(tracking_uri=MLFLOW_URI)\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.start_run(run_name=RUN_NAME)\n",
    "mlflow_run_id = mlflow.active_run().info.run_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an experiment\n",
    "\n",
    "This is where things will start to changes dramatically for each data scientist's specific use case. The main thing to note, is that for any parameter that you want to track in MLflow or is critically for outcome reproducibility, assign it explicitly. The variable name can be what ever you want, but the convention is to make it ALL_CAPS like in the examples below.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data set!\n",
    "\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and set test_size and random_state as variables that we can track later in MLflow  \n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.5\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=TEST_SIZE,random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all of our model's parameters\n",
    "\n",
    "To be exhaustive in this example we will explicitly set every parameter our model offers. It should be noted that except for `HIDDEN_LAYER_SIZES` the default values are used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION = 'relu'\n",
    "ALPHA = 0.0001\n",
    "BATCH_SIZE = 'auto'\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "EARLY_STOPPING = False\n",
    "EPSILON = 1e-08\n",
    "HIDDEN_LAYER_SIZES = (10,)\n",
    "LEARNING_RATE = 'constant'\n",
    "LEARNING_RATE_INIT = 0.001\n",
    "MAX_FUN = 15000\n",
    "MAX_ITER = 200\n",
    "MOMENTUM = 0.9\n",
    "N_ITER_NO_CHANGE = 10\n",
    "NESTEROVS_MOMENTUM = True\n",
    "POWER_T = 0.5\n",
    "RANDOM_STATE = None\n",
    "SHUFFLE = True\n",
    "SOLVER = 'adam'\n",
    "TOL = 0.0001\n",
    "VALIDATION_FRACTION = 0.1\n",
    "VERBOSE = False\n",
    "WARM_START = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Multi-layer Perceptron model\n",
    "\n",
    "clf = MLPClassifier(activation=ACTIVATION, alpha=ALPHA, batch_size=BATCH_SIZE, beta_1=BETA_1, beta_2=BETA_2,\n",
    "                    early_stopping=EARLY_STOPPING, epsilon=EPSILON, hidden_layer_sizes=HIDDEN_LAYER_SIZES,\n",
    "                   learning_rate=LEARNING_RATE, learning_rate_init=LEARNING_RATE_INIT,max_fun=MAX_FUN,\n",
    "                   max_iter=MAX_ITER, momentum=MOMENTUM, n_iter_no_change=N_ITER_NO_CHANGE,\n",
    "                   nesterovs_momentum=NESTEROVS_MOMENTUM, power_t=POWER_T,random_state=RANDOM_STATE,\n",
    "                   shuffle=SHUFFLE, solver=SOLVER, tol=TOL, validation_fraction=VALIDATION_FRACTION,\n",
    "                   verbose=VERBOSE, warm_start=WARM_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log each parameter in MLflow\n",
    "\n",
    "Use the `mlflow.log_param(Name, Value)` function for each parameter to log their values o for this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param(\"RANDOM_STATE\", RANDOM_STATE)\n",
    "mlflow.log_param(\"ACTIVATION\",ACTIVATION)\n",
    "mlflow.log_param(\"ALPHA\",ALPHA)\n",
    "mlflow.log_param(\"BATCH_SIZE\",BATCH_SIZE)\n",
    "mlflow.log_param(\"BETA_1\",BETA_1)\n",
    "mlflow.log_param(\"BETA_2\",BETA_2)\n",
    "mlflow.log_param(\"EARLY_STOPPING\",EARLY_STOPPING)\n",
    "mlflow.log_param(\"EPSILON\",EPSILON)\n",
    "mlflow.log_param(\"HIDDEN_LAYER_SIZES\",HIDDEN_LAYER_SIZES)\n",
    "mlflow.log_param(\"LEARNING_RATE\",LEARNING_RATE)\n",
    "mlflow.log_param(\"LEARNING_RATE_INIT\",LEARNING_RATE_INIT)\n",
    "mlflow.log_param(\"MAX_FUN\",MAX_FUN)\n",
    "mlflow.log_param(\"MAX_ITER\",MAX_ITER)\n",
    "mlflow.log_param(\"MOMENTUM\",MOMENTUM)\n",
    "mlflow.log_param(\"N_ITER_NO_CHANGE\",N_ITER_NO_CHANGE)\n",
    "mlflow.log_param(\"NESTEROVS_MOMENTUM\",NESTEROVS_MOMENTUM)\n",
    "mlflow.log_param(\"POWER_T\",POWER_T)\n",
    "mlflow.log_param(\"RANDOM_STATE\",RANDOM_STATE)\n",
    "mlflow.log_param(\"SHUFFLE\",SHUFFLE)\n",
    "mlflow.log_param(\"SOLVER\",SOLVER)\n",
    "mlflow.log_param(\"TOL\",TOL)\n",
    "mlflow.log_param(\"VALIDATION_FRACTION\",VALIDATION_FRACTION)\n",
    "mlflow.log_param(\"VERBOSE\",VERBOSE)\n",
    "mlflow.log_param(\"WARM_START\",WARM_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(10,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      0.57      0.72        23\n",
      "           2       0.70      1.00      0.82        23\n",
      "\n",
      "    accuracy                           0.87        75\n",
      "   macro avg       0.90      0.86      0.85        75\n",
      "weighted avg       0.91      0.87      0.86        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the performance metrics you are interested in tracking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = classification_report(y_test, y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT_DIR = 'outputs'\n",
    "#mlflow.set_tag( \"model\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log the performance metrics\n",
    "\n",
    "Using the `MLFLOW_CLIENT.log_metric(mlflow_run_id, metric_name, metric_value)` function, define each metric that you would like tracked by MLflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics \n",
    "\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_Precision \", metrics['macro avg']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_recall \", metrics['macro avg']['recall'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_f1-score \",  metrics['macro avg']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_support \",  metrics['macro avg']['support'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Accuracy \",  accuracy_score(y_test, y_pred))\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_Precision \",  metrics['0']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_recall \",  metrics['0']['recall'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_f1-score \",  metrics['0']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_support \",  metrics['0']['support'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_Precision \",  metrics['1']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_recall \",  metrics['1']['recall'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_f1-score \",  metrics['1']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_support \",  metrics['1']['support'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_Precision \",  metrics['2']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_recall \",  metrics['2']['recall'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_f1-score \",  metrics['2']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_support \",  metrics['2']['support'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish Tracking\n",
    "\n",
    "use `mlflow.end_run()` to finish the experiment and send all the logged values to the MLflow deployment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review results\n",
    "\n",
    "http://mlflow-server-route-aiops-prod-prometheus-scrape.cloud.paas.psi.redhat.com/#/experiments/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Experiments\n",
    "\n",
    "The example above assumes a one off or  hand tuning situation. What if I wanted to perform a parameter search a log all the results with MLflow. Let take all the above and make a slightly more complicated example where we run 3 different experiments, where we will very the number of hidden layers and then log the results in MLflow.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/app-root/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/app-root/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "for h in [5,10,25]:\n",
    "\n",
    "# configurations set by the user\n",
    "\n",
    "    MLFLOW_URI = 'http://mlflow-server-route-aiops-prod-prometheus-scrape.cloud.paas.psi.redhat.com' #route to deployment\n",
    "    EXPERIMENT_NAME = 'AIOps_tracking_test_0.1'# Name for entire set of experiments you want to compare. \n",
    "    RUN_NAME = \"template_example\" # custom name for this specific run\n",
    "\n",
    "    MLFLOW_CLIENT = mlflow.tracking.MlflowClient(tracking_uri=MLFLOW_URI)\n",
    "    mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    mlflow.start_run(run_name=RUN_NAME)\n",
    "    mlflow_run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "\n",
    "\n",
    "    # Load our data set!\n",
    "\n",
    "    data = load_iris()\n",
    "\n",
    "    # Split data and set test_size and random_state as variables that we can track later in MLflow  \n",
    "\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.5\n",
    "\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=TEST_SIZE,random_state=RANDOM_STATE)\n",
    "\n",
    "    ACTIVATION = 'relu'\n",
    "    ALPHA = 0.0001\n",
    "    BATCH_SIZE = 'auto'\n",
    "    BETA_1 = 0.9\n",
    "    BETA_2 = 0.999\n",
    "    EARLY_STOPPING = False\n",
    "    EPSILON = 1e-08\n",
    "    HIDDEN_LAYER_SIZES = (h,)\n",
    "    LEARNING_RATE = 'constant'\n",
    "    LEARNING_RATE_INIT = 0.001\n",
    "    MAX_FUN = 15000\n",
    "    MAX_ITER = 200\n",
    "    MOMENTUM = 0.9\n",
    "    N_ITER_NO_CHANGE = 10\n",
    "    NESTEROVS_MOMENTUM = True\n",
    "    POWER_T = 0.5\n",
    "    RANDOM_STATE = None\n",
    "    SHUFFLE = True\n",
    "    SOLVER = 'adam'\n",
    "    TOL = 0.0001\n",
    "    VALIDATION_FRACTION = 0.1\n",
    "    VERBOSE = False\n",
    "    WARM_START = False\n",
    "\n",
    "    clf = MLPClassifier(activation=ACTIVATION, alpha=ALPHA, batch_size=BATCH_SIZE, beta_1=BETA_1, beta_2=BETA_2,\n",
    "                        early_stopping=EARLY_STOPPING, epsilon=EPSILON, hidden_layer_sizes=HIDDEN_LAYER_SIZES,\n",
    "                       learning_rate=LEARNING_RATE, learning_rate_init=LEARNING_RATE_INIT,max_fun=MAX_FUN,\n",
    "                       max_iter=MAX_ITER, momentum=MOMENTUM, n_iter_no_change=N_ITER_NO_CHANGE,\n",
    "                       nesterovs_momentum=NESTEROVS_MOMENTUM, power_t=POWER_T,random_state=RANDOM_STATE,\n",
    "                       shuffle=SHUFFLE, solver=SOLVER, tol=TOL, validation_fraction=VALIDATION_FRACTION,\n",
    "                       verbose=VERBOSE, warm_start=WARM_START)\n",
    "\n",
    "    # Parameters\n",
    "    mlflow.log_param(\"RANDOM_STATE\", RANDOM_STATE)\n",
    "    mlflow.log_param(\"ACTIVATION\",ACTIVATION)\n",
    "    mlflow.log_param(\"ALPHA\",ALPHA)\n",
    "    mlflow.log_param(\"BATCH_SIZE\",BATCH_SIZE)\n",
    "    mlflow.log_param(\"BETA_1\",BETA_1)\n",
    "    mlflow.log_param(\"BETA_2\",BETA_2)\n",
    "    mlflow.log_param(\"EARLY_STOPPING\",EARLY_STOPPING)\n",
    "    mlflow.log_param(\"EPSILON\",EPSILON)\n",
    "    mlflow.log_param(\"HIDDEN_LAYER_SIZES\",HIDDEN_LAYER_SIZES)\n",
    "    mlflow.log_param(\"LEARNING_RATE\",LEARNING_RATE)\n",
    "    mlflow.log_param(\"LEARNING_RATE_INIT\",LEARNING_RATE_INIT)\n",
    "    mlflow.log_param(\"MAX_FUN\",MAX_FUN)\n",
    "    mlflow.log_param(\"MAX_ITER\",MAX_ITER)\n",
    "    mlflow.log_param(\"MOMENTUM\",MOMENTUM)\n",
    "    mlflow.log_param(\"N_ITER_NO_CHANGE\",N_ITER_NO_CHANGE)\n",
    "    mlflow.log_param(\"NESTEROVS_MOMENTUM\",NESTEROVS_MOMENTUM)\n",
    "    mlflow.log_param(\"POWER_T\",POWER_T)\n",
    "    mlflow.log_param(\"RANDOM_STATE\",RANDOM_STATE)\n",
    "    mlflow.log_param(\"SHUFFLE\",SHUFFLE)\n",
    "    mlflow.log_param(\"SOLVER\",SOLVER)\n",
    "    mlflow.log_param(\"TOL\",TOL)\n",
    "    mlflow.log_param(\"VALIDATION_FRACTION\",VALIDATION_FRACTION)\n",
    "    mlflow.log_param(\"VERBOSE\",VERBOSE)\n",
    "    mlflow.log_param(\"WARM_START\",WARM_START)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    metrics = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Metrics \n",
    "\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_Precision \", metrics['macro avg']['precision'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_recall \", metrics['macro avg']['recall'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_f1-score \",  metrics['macro avg']['f1-score'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_support \",  metrics['macro avg']['support'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Accuracy \",  accuracy_score(y_test, y_pred))\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_Precision \",  metrics['0']['precision'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_recall \",  metrics['0']['recall'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_f1-score \",  metrics['0']['f1-score'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_support \",  metrics['0']['support'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_Precision \",  metrics['1']['precision'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_recall \",  metrics['1']['recall'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_f1-score \",  metrics['1']['f1-score'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_support \",  metrics['1']['support'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_Precision \",  metrics['2']['precision'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_recall \",  metrics['2']['recall'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_f1-score \",  metrics['2']['f1-score'])\n",
    "    MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_support \",  metrics['2']['support'])\n",
    "\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review results\n",
    "\n",
    "http://mlflow-server-route-aiops-prod-prometheus-scrape.cloud.paas.psi.redhat.com/#/experiments/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
